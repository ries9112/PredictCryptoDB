[
["index.html", "Predict Crypto Database Quick Start Guide 1 Overview", " Predict Crypto Database Quick Start Guide Ricky Esclapon - riccardo.esclapon@colorado.edu 2020-06-25 1 Overview This is a quick start guide for the Predict Crypto DataBase which should provide the support you need to interact with the database and pull data. Everything you need to know will be outlined in this document and you can use the sidebar on the left (s is the hotkey to show/hide it) to review the following sections: Overview- This section. Interacting with the DB- Instructions around accessing the Metabase environment that will allow you to interact with the database, which is hosted on the website PredictCryptoDB.com Useful Tables- A review of some of the more useful/interesting tables you can find within the database and an overview of the best places to get started. Usage Guide- This guide explores some of the functionality found within the website/Metabase environment. Documentation Usage- An overview of the documentation that is available through the Metabase environment and how to use it to answer questions you may have around where the data is sourced from as well as a complete data dictionary for every field in every table. Additional Tips- Some additional notes around using the environment to its full potential. This section goes over things like pulling the most recent data, creating dashboards, embedding anything you create as an iframe within a website or blog post, creating e-mail triggers and using a Python package to execute trades. Development Roadmap- This section provides an overview of what tables I am looking to add into the PredictCrypto database next as well as dates on when the new sources will be added. You can toggle the sidebar on the left side of the screen by pressing the letter s on your keyboard. "],
["interacting-with-the-db.html", "2 Interacting with the DB 2.1 Benefits 2.2 “Metabase” explained 2.3 Logging In", " 2 Interacting with the DB First off, why the heck is there a website? If there’s a database why wouldn’t you just access it through a SQL editor like SSMS or MySQL? Let’s start with the benefits of having this environment readily available to access through a website before jumping into how to start interacting with the database. 2.1 Benefits Giving access to the database can be challenging, and having a centralized server that can be accessed through a website allows us to analyze the data without worrying about individual IP addresses being allowed access and other issues we would run into relating to authentication without this type of environment to work out of. There is no setup required on your part to start working with the database and as soon as your account is setup you can start writing SQL queries through the website and log into the website at any time. You can access the Metabase interface to interact with the database from any device, like a tablet or smartphone. It works as a shared environment where we can all collaborate and see each other’s dashboards, sql queries, visualizations etc… This is similar to the way a Tableau Online website would work within an organization. Makes creating dashboards to visualize the data extremely simple. Those dashboards can then be shared and be embedded, and the data can be refreshed on a schedule in a matter of clicks. 2.2 “Metabase” explained First, let me clarify on what I mean by Metabase and how this all works. Metabase is the name of the open source software that we are using to interact with the database. Using the docker container available to download on Metabase’s website, I stood up a server-type environment in AWS that hosts the Metabase instance that is connected to the database. The AWS environment running the Metabase instance can be connected to by opening your web browser and navigating to the https secured website predictcryptodb.com. From here, you will use the set of credentials that you created after clicking on the invitation from the e-mail you received. If you need an invite to create a set of credentials, e-mail me at riccardo.esclapon@colorado.edu. 2.3 Logging In To get started, the first thing you will want to do every time is go to the URL predictcryptodb.com. You should see a login page: If you are having issues accessing the website, try spelling the url including https https://predictcryptodb.com. Before you can login, you need to configure your account through the e-mail you should have received. If you did not receive the e-mail, check your spam folder first: If you need the e-mail to setup your login, e-mail me Configure your password and you will be able to use your e-mail and password to login in the future: Once you have logged in, you should see a page that looks like this: In the next section we will walk through some useful tables found in the database to get you started "],
["useful-tables.html", "3 Useful Tables 3.1 Browse Data 3.2 Useful tables", " 3 Useful Tables In this section we will be reviewing some interesting tables and good places to get started. 3.1 Browse Data Let’s start by clicking on the Browse Data tab in the top right of the Metabase environment: Here, you should see the options Octoparse and ScrapeStorm: Octoparse is the schema that is associated with data I have collected by using the Octoparse web scraping software. Conversely, ScrapeStorm is the schema associated with data collected using the ScrapeStorm web scraping software. You should also see an option for PredictCryptoPredictions. This schema does not have much in it right now, but over time as I do more predictive modeling it will populate with new tables for new predictive models and is used to simulate model performance before starting to programmatically trade using the predictions. This guide ignores this schema/database for now to focus on the raw data itself, which always comes from the databases Octoparse and ScrapeStorm. Back in MetaBase, let’s click on the option that says Octoparse: I would recommend starting here because this was the first/original database and will have more historical data compared to ScrapeStorm, which I got up and running much later Now you should see the tables that are contained within the Octoparse schema. By hovering over each table, you will see three options appear, which will be better explained in the next section about Documentation Usage. In the screenshot the mouse is hovering over the i symbol for the Bitgur table: By clicking on the middle button that says Learn more about this table, you will be brought to its documentation: For now, let’s go ahead and click on the name of the table Bitgur: After clicking on the table name, you should see some example data show up. This shows the first 2,000 rows of data found in the table: In the next section Usage Guide we will walk through some of the functionality associated with the things circled in red in the screenshot above using the Bitgur table as an example. 3.2 Useful tables For the previews below, keep an eye out for a button to show more columns: Things will tend to live as chr/strings within the database because I found that saving everything as a string prevents schema conflicts from no longer uploading data to the database after it gets collected. The previews below will show you the data types as well, so just keep in mind you might have to change the data types sometimes after extracting the data from the database. The data shown below should be no more than 1 day old. The latest data is shown for each table and this document refreshes automatically daily. 3.2.1 Tables in Octoparse db All date/time fields in the Octoparse database are in UTC 3.2.1.1 Bitgur ## Warning in result_create(conn@ptr, statement, is_statement): Cancelling previous ## query Data collected since: 2018-11-11 Documentation: predictcryptodb.com/reference/databases/2/tables/36 3.2.1.2 BitgurPerformance ## Warning in result_create(conn@ptr, statement, is_statement): Cancelling previous ## query Data collected since: 2019-11-05 Documentation: predictcryptodb.com/reference/databases/2/tables/60 3.2.1.3 CoinCheckup Data collected since: 2018-12-04 Documentation: predictcryptodb.com/reference/databases/2/tables/587 3.2.1.4 CoinCheckupDetails Data collected since: 2019-10-01 Documentation: predictcryptodb.com/reference/databases/2/tables/591 3.2.1.5 CoinStatsPrices Data collected since: 2018-11-11 Documentation: predictcryptodb.com/reference/databases/2/tables/28 3.2.1.6 CoinToBuy ## Warning in result_create(conn@ptr, statement, is_statement): Cancelling previous ## query Data collected since: 2018-11-16 Documentation: predictcryptodb.com/reference/databases/2/tables/26 3.2.1.7 TechnicalAnalysis ## Warning in result_create(conn@ptr, statement, is_statement): Cancelling previous ## query Data collected since: 2018-11-15 Documentation: predictcryptodb.com/reference/databases/2/tables/61 3.2.2 Tables in ScrapeStorm db Any date/time fields in the ScrapeStorm database are in the MST timezone (Colorado time) 3.2.2.1 Messari Data collected since: 2019-09-26 Documentation: predictcryptodb.com/reference/databases/4/tables/479 3.2.2.2 ShrimpyPrices &lt;!-- ![Example for the KuCoin exchange](images/ShrimpyKucoin.png) --&gt; Data collected since: 2019-11-16 Documentation: predictcryptodb.com/reference/databases/4/tables/467 3.2.2.3 ShrimpyPricesBTC ## Warning in result_create(conn@ptr, statement, is_statement): Cancelling previous ## query Data collected since: 2019-11-16 Documentation: predictcryptodb.com/reference/databases/4/tables/454 3.2.3 PredictCryptoPredictions db For the PredictCrypto project, I have been working on different iterations of the predictive models to predict and trade on the live cryptocurrency markets. For an overview of what this process looks like from start to finish, please see the Alteryx Use Case for the project: https://community.alteryx.com/t5/Alteryx-Use-Cases/Predicting-and-Trading-on-the-Cryptocurrency-Markets-using/ta-p/494058 As I improve things on the predictive modeling side of things, I am going to create different iterations of the model and write out predictions made in real time by the newest models and save those predictions so I can analyze what would have happened by actually trading on them. Once I have done more progress I will provide more tools and better documentation around analyzing the performance of the different predictive models. 3.2.4 Database size info Size in MB of both the Octoparse database and the ScrapeStorm db as of the last time this document was refreshed (updated daily): DB name Size in MB Today Octoparse 37,065.203 2020-06-25 ScrapeStorm 4,251.438 2020-06-25 PredictCryptoPredictions 1,859.141 2020-06-25 Number of rows by table: ## Warning in result_create(conn@ptr, statement, is_statement): Cancelling previous ## query Database Table Name Rows Octoparse Bitgur 28136580 Octoparse BitgurPerformance 6111712 Octoparse CoinToBuy 4642794 ScrapeStorm SymbolsMessariJoin 4302084 ScrapeStorm Messari 3107309 ScrapeStorm ShrimpyPricesBTC 3094028 ScrapeStorm BitgurBackup 2784959 Octoparse TechnicalAnalysis 2741390 Octoparse Messari 1887489 ScrapeStorm ShrimpyPrices 1862532 ScrapeStorm Shrimpy_R 1354580 Octoparse Shrimpy_R 1323020 Octoparse CoinToBuyCountries 1237147 Octoparse CoinCheckup 1070160 ScrapeStorm Shrimpy_Orderbook 787538 ScrapeStorm GithubActivity_9months 643954 ScrapeStorm KuCoinPrices 599051 ScrapeStorm GithubActivity_12months 564779 ScrapeStorm GithubActivity_6months 511644 ScrapeStorm GithubActivity_3months 437692 Octoparse CoinCheckupDetails 186314 ScrapeStorm Messari_R 162119 Octoparse Messari_R 147388 3.2.5 Why two web scraping tools? Web scraping has its challenges in terms of stability, so I built some additional resilience by using two different tools that work independently of each other and do similar things (and in some cases collect the same data). Although not a perfect solution, having both up and running means we can usually fill the gaps that might arise in each tool respectively. "],
["usage-guide.html", "4 Usage Guide 4.1 Answer Questions 4.2 pkey and pkDummy fields 4.3 Tabular Data 4.4 Creating dashboards 4.5 Using x-rays to explore a table 4.6 Sharing", " 4 Usage Guide Now that you’ve familiarized yourself with the tables found in the previous section, this next section is going to help you understand how to pull data out of the database and how to join different tables together and being able to put that all together in more useful formats and outputs. 4.1 Answer Questions We now have a specific table in mind that we are looking to analyze, and maybe even a specific analysis in mind. To get started, click on the option in the top right Ask a question: Here, you will be given three different options: Simple question After choosing your table you will be brought to an interactive view of the data. See the section about tabular data for more information around using this methodology. Custom question When picking this option, the underlying functionality is similar to what is given on the simple question option, but when doing a custom question the data won’t Native Query If you’re comfortable in SQL and you find that the Metabase interface just gets in the way, feel free to write your own raw SQL code by selecting this option. I have limited the Metabase environment to only allow for SELECT statements, if you need additional privileges send me an e-mail. In summary, if you are not familiar with a table and you’re looking to explore the data as you filter different values and whatnot, you will probably want to pick the Simple question. If you have a specific analysis in mind already (e.g. last 7 days prices for ETH sorted by DateTime), you might get to your answer faster making a Custom question so you’re waiting less for the data to load. If you are looking to write your own SQL directly instead of using the Metabase environment to help explore the data in a more intuitive way, go ahead and use the Native Query option. 4.1.1 Exporting Data Before going over how to export the data, just a quick word about the confidentiality of the data. Over time I have grown to like the idea of making this data more accessible rather than keeping it as private as possible, BUT I have spent a lot of money and time working on this and I would like any type of sharing of the data to go through me for now. Feel free to download as much data out of the database as you would like, just please do not share this data with people who I haven’t explicitly given access to. To get someone access, please have them e-mail me at riccardo.esclapon@colorado.edu directly. In order to export the data you are pulling through Metabase, you would press the Download button in the bottom right of the page: When you are working within Metabase, if your results produce more than 2,000 rows, you would only see a preview of the first 2,000 rows. When you download the results however, you will download the full results of your query up to a million rows. For example, if you were doing a raw SQL query SELECT * FROM Bitgur in the Octoparse database, you would see the first 2,000 rows of the Bitgur raw data, but if you were to click on the Download button, you would be downloading the first 1,000,000 rows from the Bitgur dataset (since it has more than 1 million rows). If you adjusted the query to be SELECT * FROM Bitgur ORDER BY Date DESC and you clicked Download, you would be extracting the most recent 1,000,000 rows of data out of the database. If you are looking to train a model or do something that requires you to extract more than 1,000,000 rows and you are having a hard time with that limit send me an e-mail. 4.2 pkey and pkDummy fields Let’s say you went ahead and trained a predictive model using the data found in the database and now you want to test making some semi-live predictions, how would you go about pulling the latest data to make predictions on? A row is uniquely identified by the individual cryptocurrency symbol, and the date and hour of when the data was collected. All tables were given values that help uniquely identify a data point. All these values are doing, is taking the two values that uniquely identify a row in the dataset and saving the result in a new field called pkey. The first step to calculating the pkey field is to create a field called pkDummy, which is a string that gives the Date and the hour of the day, but no information about the minutes or seconds. For example, if the DateTime of when the data was extracted is 2020-01-01 10:03:53, the resulting value found within the pkDummy field would be 2020-01-01 10. Once the pkDummy field is being properly populated, there is a generated column that is found within all tables called pkey, which is calculated through this formula: concat(pkDummy,Symbol), meaning the example from earlier would have the following pkey for the row associated with Bitcoin: 2020-01-01 10BTC. You can now use the pkey 2020-01-01 10BTC in order to join data from a different table that was also collected on January 1st, 2020 at 10am by a different table. Keep in mind that these joins will not be perfect because the data will inevitably differ in terms of when it was specifically collected. Every table also has a field showing the full DateTime of when the data was extracted, so always keep an eye on that field and make sure the data you are joining was collected no longer than a couple of minutes apart. If you are looking to pull the latest/live data from the current hour, take the maximum of the pkDummy field max(pkDummy). In my experience this works fine, but if the data type ends up giving problems I can convert this to a big integer with no dashes and the order of the numbers would then work (e.g. 2020010110 &gt; 2019123010). Important Note: Only do this for tables that are found within the same database! Currently, you can only use the pkey to tie information from tables found within the Octoparse database to other tables within the Octoparse database, and not the ScrapeStorm database. This is because the data that flows into the Octoparse database is always recorded with the UTC timezone, while the data that flows into the ScrapeStorm database is always in the MST Colorado timezone. Because MST has daylight savings and UTC does not, there is no hard-coded adjustment that can be applied here, like subtracting 6 hours. The only effective way that I found to properly adjust the timezones and to enable a join between the different databases is by using the Alteryx formulas DateTimeToLocal() and DateTimeToUTC(): After making this adjustment to the table you are looking to join, you would need to apply the adjustment to the pkDummy and pkey fields as well, since those are whate gets used for the join and not the raw DateTime field. In this case we are adjusting the pkey for a table found in the ScrapeStorm database that we want to join to data found in the Octoparse database, because we are going from MST time (associated with ScrapeStorm db) to UTC time (associated with Octoparse db). 4.3 Tabular Data Within Metabase when you start working with a table, you will usually see a preview of the raw data. From there, you have several options in terms of what you can do apart from exporting the results. The first thing I want to point out, is that you can click on each field to open a menu with an assortment of options based on the data type. For example, if we click on the Price field, these are the options we are given: The Date field is a different data types and offers different options. For example we could sort the data by the Date field Descending: Which would return the first 2,000 rows again but this time sorted showing the largest values first (meaning the latest date): If the Bitgur table is not being very responsive, try using the BitgurPerformance table instead, which should load faster (but has less historical data, this table is described in a section below) Next, let’s click on the purple Filter button in the top right of the page: Now you should see the following sidebar pop-up: Notice how the icons next to the field names correspond to their data types Let’s do a quick example filtering based on the DateTime field: Let’s add a filter to only select data where the DateTime field is from the last 7 days: please be patient as this loads or do the same with a different table that contains less historical data Now that we have filtered the data down to the subset we are interested in, we could adjust the look of the table using the Settings option in the bottom left: From here there are lots of customization options for the way the table looks 4.3.1 Creating Visualizations If you are looking to create a visual representation of the data, you can do so by pressing on the button in the bottom left of the page: The tabular view is a visualization option itself as well For a complete description of all visualization options, please see the excellent official Metabase documentation on the topic: https://www.metabase.com/docs/latest/users-guide/05-visualizing-results 4.4 Creating dashboards Start by clicking on the + sign in the top right of the page and selecting the option New Dashboard: When prompted fill out the Name and Description for the table and create the dashboard: Now you will need to start adding questions to your dashboard: Add as many questions as you want to your dashboard and create whatever layout you would like. Once you’re finished adding questions, add a filter shared by the dashboards. To do this, first enter the Edit mode of the dashboard, and select the Filter option: For example, you could be showing the last 7 days of data for Ethereum only across all questions (tables/charts) being shown on the dashboard: You can set the dashboard to auto-refresh on a schedule as well: 4.4.1 Example dashboard Click here for a very simple example of a dashboard. 4.5 Using x-rays to explore a table In the previous sections we have gone over how you would start to understand the fields collected by a table, but we haven’t talked too much about exploring the actual values found within a table. For example, we might be able to have a good sense of what the field Volatility may mean, but we wouldn’t get a good understanding of the range of values for that variable and it’s distribution. A really good way to answer these questions, is by taking an x-ray of the table. When taking an x-ray of a table Metabase does a quick scan of the values found within the table and creates a preview of those values. It will tell you the total number of rows found in the table, the number of new rows in the past 30 days, a distribution of the values for each variable in the table, and a preview of how the values have changed over time: Summary of table and distributions. Right-skew becomes easy to spot for the Index variable Field values over time The database does not have nearly as many dedicated resources as a database at a big company would in order to stay as cost-effective as possible, but if you stay patient usually Metabase is able to create an x-ray of the table, and when you go back to it later on it should take less to load because it is able to retrieve some cached data. 4.6 Sharing Once you have created something like a dashboard that auto-refreshes periodically, it can be shared with others as a private link, or be embedded directly within a website. Almost everything you create in the Metabase environment should have a Share button, that on hover says Sharing and embedding: Once you press the button, you’ll have the option of enabling or disabling a link that can be used to publicly access this information without a login. Please be mindful of the privacy of the data whenever you share anything, and when in doubt just shoot me a message. "],
["documentation-usage.html", "5 Documentation Usage 5.1 pkDummy and pkey 5.2 Addtional Notes", " 5 Documentation Usage Everything you have visibility to within the Metabase environment is documented. Meaning, every table you see has a description which includes the URL of where the data is collected (as well as the web scraping methodology where informative), and every field within those tables has also been documented on predictcryptodb.com. The best way to access the documentation just described, is by doing the following: Click on the button in the top right with the console symbol that says Write SQL when hovered over: Click the button on the new window that looks like a book that says Learn about your data when hovered over: Now a sidebar should appear from the right side of the page, which I have found to be by far the best way to interact with the Data Reference: You can click into any Table, and from there you can click into any field to easily retrieve the definition of the specific field. You can keep the documentation open as you write your SQL queries: 5.1 pkDummy and pkey The pkDummy and pkey fields are particularly important, so in case you missed it, go back and review this section from the usage guide that explains these two fields. 5.2 Addtional Notes Every table you have visibility to also has an e-mail alert system that alerts me if the data stops flowing through on an hourly (or daily for some) basis. If anything becomes broken I will remove the visibility of that table within Metabase, so if a table is found within the Metabase environment you can assume the web scraper for that table is still functioning correctly and that the data is uploaded to the table on an automated schedule. Any table that starts with the text AYX followed by a bunch of random letters and numbers, is a Temporary Table that was made through Alteryx. Please ignore these tables completely, as they will disappear over time. "],
["additional-tips.html", "6 Additional Tips 6.1 Sharing anything created on Metabase 6.2 E-mail Alerts 6.3 Creating Variables for queries 6.4 Using the Shrimpy-Python Library", " 6 Additional Tips This section goes over some additional tips around using Metabase. 6.1 Sharing anything created on Metabase See this section regarding sharing things that you create in Metabase. 6.2 E-mail Alerts The Metabase environment is setup to allow for e-mail alerts, where you can e-mail out results on a schedule. 6.2.1 Existing e-mail alerts I have used this feature of the Metabase environment to setup alerts to ensure the data is flowing in as expected into the database. Every hour a set of quick queries runs to check if any of the data is more outdated than it should be. For tables where the data flows in every hour, it checks that the latest data is not older than one hour. If the latest data is older than one hour, I get an e-mail letting me know for each table individually. At the start of every hour, there is an alert that checks for queries that have been running longer than 20 minutes and e-mails me the SQL statements to kill those queries. 6.2.2 Creating e-mail alerts When creating a table or chart in Metabase, you will have the option in the bottom right of the page to create an e-mail alert by clicking on the button shaped like a bell: From here the menu to setup the e-mail alerts should be very straightforward: For more thorough instructions around creating e-mail triggers, review the official Metabase documentation: https://www.metabase.com/docs/latest/users-guide/15-alerts.html 6.2.3 Pulses Pulses are almost exactly like e-mail alerts, see the official Metabase documentation for details around using pulses: https://www.metabase.com/docs/latest/users-guide/10-pulses 6.3 Creating Variables for queries Within a new question, click on the “x” symbol to open sidebar on the side and learn how to use variables: {{variable_name}} creates a variable in this SQL template called “variable_name”. Variables can be given types in the side panel, which changes their behavior. All variable types other than “Field Filter” will automatically cause a filter widget to be placed on this question; with Field Filters, this is optional. When this filter widget is filled in, that value replaces the variable in the SQL template. 6.4 Using the Shrimpy-Python Library This section is a bit more expert level and I don’t really expect anyone to go down this route, but I figured it was worth having a small section on using the Shrimpy software for trading cryptocurrencies by using their Python package. This is a particularly good way to execute trades on the cryptocurrency markets because rather than having to connect to and manage each API individually, you can connect up to 16 exchanges to your Shrimpy account and programmatically trade on every one of them using the same set of functions in Python. First you would have to import the shrimpy-python library. For installation instructions see this link ## Warning in connection_release(conn@ptr): There is a result object still in use. ## The connection will be automatically released when it is closed "],
["development-roadmap.html", "7 Development Roadmap 7.1 Data Sources 7.2 Tutorials", " 7 Development Roadmap 7.1 Data Sources 7.1.1 February 1, 2020 By this date I will have added the following web scrapers: New web scraper for: cryptofinance.ai/ Legally acquire data from CoinMarketCap.com through the service New web scraper for: stocktwits.com/symbol/BTC Extract sentiment scores and change New web scraper to iterate through list of cryptocurrencies, e.g.: [www.marketwatch.com/investing/cryptocurrency/btcusd](https://www.marketwatch.com/investing/cryptocurrency/btcusd Extract Flipside ratings and change New web scraper to extract news articles to start doing more semantic analysis oriented work: www.thestreet.com/investing/cryptocurrency Another new web scraper to extract news articles: www.marketwatch.com/investing/cryptocurrency 7.1.2 March 1, 2020 By this date I will have finalized the new process in Alteryx to run the v3 version of the XGBoost models. 7.1.3 March 15, 2020 By March 15th, the new v3 version of the models will be executing trades on the cryptocurrency markets in an automated fashion. 7.2 Tutorials 7.2.1 January 25, 2020 By this date I will make available the first set of programming tutorials on PredictCrypto.com. These will comprise of: A tutorial on the basics of R. Not crypto oriented but still uses live crypto data. An interactive presentation where the user gets to make selections (like what cryptocurrency to analyze) and based on those selections gets relevant practice problems relating to the content found in the first tutorial about the basics of R. A slightly more advanced (intermediate level) tutorial in R. This tutorial however will take a different approach than the first one and will walk through making predictions on the cryptocurrency markets from start to finish. This tutorial uses the last 7 days of data from whenever the tutorial is launched. Another interactive presentation similar to #2 from earlier, but this time giving practice problems for the more advanced R tutorial. 7.2.2 April 1-April 25, 2020 This is the only thing that is not related to cryptocurrencies that I am adding to the roadmap ahead. On April 1st 2020 the US will undergo its census, which only happens once every 10 years and will provide one of the best datasets in the world for free. Therefore from April 1st to around the 25th I will plan to only be working on creating interactive ways to analyze the census data as well as tutorials associated with the dataset. 7.2.3 June 1, 2020 By this date I would like to create tutorials using spark and R. Spark can be particularly challenging to learn becuase there’s no real good way to practice without connecting to a spark cluster and that can be challenging and expensive. I think I could create a solution that allows people to learn Spark using data from crypto markets the same way I did for the SQL database and it would cost somewhere around $150 a month to maintain. I think that would be worth it, but I need to evaluate that when the time comes. I think this would be one of the more valuable learning tools if created. 7.2.4 August 1, 2020 Around this date I plan on expanding the technologies taught through the tutorials made available on PredictCrypto.com to also include Python, and ideally some Javascript as well. 7.2.5 November 1, 2020 Around this date ideally I have started working on creating my own blockchain from scratch and have started to work on tutorials that teach about blockchain. 7.2.6 December 1, 2020 Around the end of the year ideally I could find the time to dive into deeper concepts and providing more digestible tutorials around more complex topics like creating very advanced Neural Networks (like having a neural network that creates the structure of the neural network used to make predictions), NLP, AI, blockchain, etc… "]
]
